{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Static Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import requests\n",
    "import re\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constant mapping between exercise names and their root derivation trees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_to_model_ids = {\n",
    "    'Courses': ['JDKw8yJZF5fiP3jv3', 'PSqwzYAfW9dFAa9im'],\n",
    "    'ProductionLine_v2_v3': ['aTwuoJgesSd8hXXEP', 'bNCCf9FMRZoxqobfX'],\n",
    "    'Train': ['QxGnrFQnXPGh2Lh8C'],\n",
    "    'SocialNetwork': ['dkZH6HJNQNLLDX6Aj'],\n",
    "    'TrashFOL': ['sDLK7uBCbgZon3znd'],\n",
    "    'ClassroomFOL': ['YH3ANm7Y5Qe5dSYem'],\n",
    "    'TrashRL': ['PQAJE67kz8w5NWJuM'],\n",
    "    'ClassroomRL': ['zRAn69AocpkmxXZnW'],\n",
    "    'Graphs': ['gAeD3MTGCCv8YNTaK'],\n",
    "    'LTS': ['zoEADeCW2b2suJB2k'],\n",
    "    'ProductionLine_v1': ['jyS8Bmceejj9pLbTW'],\n",
    "    'CV': ['JC8Tij8o8GZb99gEJ'],\n",
    "    'TrashLTL': ['9jPK8KBWzjFmBx4Hb']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_http_request(url: str, body=None, method=\"POST\"):\n",
    "    try:\n",
    "        response = requests.request(method, url, json=body)\n",
    "        # Check if the request was successful (status code 200)        \n",
    "        if response.status_code == 200:\n",
    "            content_type = response.headers.get('Content-Type')\n",
    "            if content_type and 'application/json' in content_type:\n",
    "                try:\n",
    "                    return response.json()\n",
    "                except requests.exceptions.JSONDecodeError as e1:\n",
    "                    return response.text\n",
    "            else:\n",
    "                return response.text\n",
    "\n",
    "        else:\n",
    "            print(f\"Request exited with status code {response.status_code}: {response.reason}\")\n",
    "    except requests.RequestException as e:\n",
    "        return e\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup databases for SpecAssistant and HiGenA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Setup HiGenA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (name,ids) in name_to_model_ids.items():\n",
    "    send_http_request(url=\"http://localhost:8080/hint/higena-setup\",  body=ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup SpecAssistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request exited with status code 204: No Content\n"
     ]
    }
   ],
   "source": [
    "send_http_request(url=\"http://localhost:8080/hint/debug-drop-db\", method=\"GET\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_specassistant_setup_from_name(name):\n",
    "    if (name in name_to_model_ids):\n",
    "        return send_http_request(url=\"http://localhost:8080/hint/specassistant-setup?prefix=\"+name, body=name_to_model_ids[name], method=\"GET\")\n",
    "    else:\n",
    "        return \"Unkown Exercise\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Setup Single"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup Classroom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOL\n",
    "send_specassistant_setup_from_name(\"ClassroomFOL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RL\n",
    "send_specassistant_setup_from_name(\"ClassroomRL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup Social Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setup SpecAssistant Desired Graphs\n",
    "send_specassistant_setup_from_name(\"SocialNetwork\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup completed for Courses with model_ids [JDKw8yJZF5fiP3jv3, PSqwzYAfW9dFAa9im]\n",
      "Setup completed for ProductionLine_v2_v3 with model_ids [aTwuoJgesSd8hXXEP, bNCCf9FMRZoxqobfX]\n",
      "Setup completed for Train with model_ids [QxGnrFQnXPGh2Lh8C]\n",
      "Setup completed for SocialNetwork with model_ids [dkZH6HJNQNLLDX6Aj]\n",
      "Setup completed for TrashFOL with model_ids [sDLK7uBCbgZon3znd]\n",
      "Setup completed for ClassroomFOL with model_ids [YH3ANm7Y5Qe5dSYem]\n",
      "Setup completed for TrashRL with model_ids [PQAJE67kz8w5NWJuM]\n",
      "Setup completed for ClassroomRL with model_ids [zRAn69AocpkmxXZnW]\n",
      "Setup completed for Graphs with model_ids [gAeD3MTGCCv8YNTaK]\n",
      "Setup completed for LTS with model_ids [zoEADeCW2b2suJB2k]\n",
      "Setup completed for ProductionLine_v1 with model_ids [jyS8Bmceejj9pLbTW]\n",
      "Setup completed for CV with model_ids [JC8Tij8o8GZb99gEJ]\n",
      "Setup completed for TrashLTL with model_ids [9jPK8KBWzjFmBx4Hb]\n"
     ]
    }
   ],
   "source": [
    "# WARNING: This takes some time and will run in the foreground, setting up every graph, one at a time\n",
    "for name in name_to_model_ids.keys():\n",
    "    print(send_specassistant_setup_from_name(name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Database Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo_uri = \"mongodb://localhost:27017/\"\n",
    "database_name = \"meteor\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Database Aggregation Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_graph_id_dict_pipeline():\n",
    "    \"\"\"Targets \"Graph\" collection\"\"\"\n",
    "    return [\n",
    "        {'$addFields': {'reg': {'$regexFind': {'input': '$name', 'regex': re.compile(r\"([^-]*)-(.*)\")}}}}, # Apply a regex find using the provided regex\n",
    "        {'$addFields': {'super_name': {'$first': '$reg.captures'}}}, # Place the first regex group on field super_name\n",
    "        {'$group': {'_id': '$super_name', 'graph_ids': {'$push': '$_id'}}} # Group based on super_name, accumulate the graph ids in the arrays graph_ids\n",
    "    ]\n",
    "    \n",
    "def get_popular_nodes_pipeline(graph_ids):\n",
    "    \"\"\"Targets \"Node\" collection\"\"\"\n",
    "    return [\n",
    "        {'$match': {'valid': False, 'graph_id': {\"$in\":graph_ids}}}, # Match invalid nodes with one of the provided graph ids\n",
    "        {'$addFields': {'formula': {'$objectToArray': '$formula'}}}, # Covert the formula object into an array of key value objects.\n",
    "        {'$unwind': '$formula'}, # Unwind object the formula (since every array is a singleton this will just update each document)\n",
    "        {'$match': {'formula.v': {'$ne': ''}}}, # Match non blank formulas (aka remove initial nodes)\n",
    "        {'$lookup': {'from': 'Node', 'localField': 'minSolution', 'foreignField': '_id', 'as': 'minSolutionFormula', \n",
    "          # A \"join\" with the document Node, values are placed in the array minSolutionFormula\n",
    "            'pipeline': [ # Pipeline applied to the \"joined\" document\n",
    "                {'$addFields': {'formula': {'$objectToArray': '$formula'}}}, # Covert the formula object into an array of key value objects.\n",
    "                {'$unwind': '$formula'}, # Unwind object the formula (since every array is a singleton this will just update each document)\n",
    "                {'$replaceRoot': {'newRoot': '$formula'}} # replace the object with its formula key value object (k,v)\n",
    "            ]\n",
    "        }},\n",
    "        {'$unwind': '$minSolutionFormula'}, # Unwind the minSolutionFormula array (since only one object can be present there wont be any new documents)\n",
    "        {'$project': { # Rewrite the object as the defined fields\n",
    "            '_id':0, # means remove id field\n",
    "            'predicate': '$formula.k', \n",
    "            'formula': '$formula.v',\n",
    "            'frequency': '$visits',\n",
    "            'closest_solution': '$minSolutionFormula.v', \n",
    "            'closest_solution_edit_distance': '$minSolutionTed'\n",
    "        }},\n",
    "        {'$sort': {'frequency': -1}} # Sort from biggest to smallest based on the defined frequency\n",
    "    ]\n",
    "    \n",
    "def get_min_solutions_pipeline(graph_ids):\n",
    "    \"\"\"Targets \"Node\" collection\"\"\"\n",
    "    return [\n",
    "        {'$match': { 'graph_id': {\"$in\":graph_ids}}}, # Match nodes with one of the provided graph ids\n",
    "        {'$group': {'_id': '$minSolution','count': {'$sum': 1}}}, # Count the minimum solution frequency\n",
    "        {'$lookup': {'from': 'Node', 'localField': '_id', 'foreignField': '_id', 'as': 'node', \n",
    "            # A \"join\" with the document Node, values are placed in the array minSolutionFormula\n",
    "            'pipeline': [ # Pipeline applied to the \"joined\" document\n",
    "                {'$addFields': {'formula': {'$objectToArray': '$formula'}}}, # Covert the formula object into an array of key value objects.\n",
    "                {'$unwind': '$formula'}, # Unwind object the formula (since every array is a singleton this will just update each document)\n",
    "            ]\n",
    "        }},\n",
    "        {'$unwind': '$node'},\n",
    "        {'$project': { # Rewrite the object as the defined fields\n",
    "            '_id': 0,  # means remove id field\n",
    "            'predicate': '$node.formula.k', \n",
    "            'formula': '$node.formula.v', \n",
    "            'frequency': '$node.visits',\n",
    "            'frequency_as_the_closest_solution':'$count'\n",
    "        }},\n",
    "        {'$sort': {'frequency': -1}} # Sort from biggest to smallest based on the defined frequency\n",
    "    ]\n",
    "\n",
    "\n",
    "def get_graph_node_statistics():\n",
    "    \"\"\"Targets \"Node\" collection\"\"\"\n",
    "    return [\n",
    "        {'$group': { # Group\n",
    "            '_id': '$graph_id', #By graph_id\n",
    "            'valid_nodes': {'$sum': {'$cond': ['$valid', 1, 0]}}, # Count valids\n",
    "            'invalid_nodes': {'$sum': {'$cond': ['$valid', 0, 1]}},  # Count invalids\n",
    "            'valid_submissions': {'$sum': {'$cond': ['$valid', '$visits', 0]}}, # Sum valid frequencies\n",
    "            'invalid_submissions': {'$sum': {'$cond': ['$valid', 0, '$visits']}} # Sum invalid frequencies\n",
    "        }},\n",
    "        {'$lookup': {'from': 'Graph', 'localField': '_id', 'foreignField': '_id', 'as': 'graph'}}, # Lookup the graph's specification and place it in an array\n",
    "        {'$unwind': '$graph'}, # Unwind the graph array\n",
    "        {'$project': {  # Rewrite the object as the defined fields\n",
    "            '_id': 0, # means remove id field\n",
    "            'name': '$graph.name', \n",
    "            'valid_formulas': '$valid_nodes', \n",
    "            'invalid_formulas': '$invalid_nodes', \n",
    "            'valid_submissions': '$valid_submissions', \n",
    "            'invalid_submissions': '$invalid_submissions'\n",
    "        }},\n",
    "        {'$sort': {'name': 1}} # Sort from biggest to smallest based on the name\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get GraphId Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient(mongo_uri)\n",
    "db = client[database_name]\n",
    "\n",
    "graph_collection = db[\"Graph\"]\n",
    "\n",
    "name_to_graph_ids = {} \n",
    "\n",
    "for doc in graph_collection.aggregate(get_graph_id_dict_pipeline()):\n",
    "    name_to_graph_ids[doc[\"_id\"]] = doc[\"graph_ids\"]\n",
    "\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Graph Stats Data Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient(mongo_uri)\n",
    "db = client[database_name]\n",
    "\n",
    "node_collection = db[\"Node\"]\n",
    "\n",
    "data = list(node_collection.aggregate(get_graph_node_statistics()))\n",
    "\n",
    "graph_stats_df = pd.DataFrame(data)\n",
    "\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Popular Node Data Frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WARNING: Requires GraphId Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient(mongo_uri)\n",
    "db = client[database_name]\n",
    "\n",
    "node_collection = db[\"Node\"]\n",
    "\n",
    "name_to_pop_dfs = {}\n",
    "\n",
    "for (name,graph_ids) in name_to_graph_ids.items():\n",
    "    data = list(node_collection.aggregate(get_popular_nodes_pipeline(graph_ids)))[0:30] # Limits output to first 30 entries\n",
    "    df_ = pd.DataFrame(data)\n",
    "    name_to_pop_dfs[name] = df_\n",
    "\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Min Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WARNING: Requires GraphId Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient(mongo_uri)\n",
    "db = client[database_name]\n",
    "\n",
    "node_collection = db[\"Node\"]\n",
    "\n",
    "name_to_min_sol_dfs = {}\n",
    "\n",
    "for (name,graph_ids) in name_to_graph_ids.items():\n",
    "    data = list(node_collection.aggregate(get_min_solutions_pipeline(graph_ids)))\n",
    "    df_ = pd.DataFrame(data)\n",
    "    name_to_min_sol_dfs[name] = df_\n",
    "\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write All Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write As Multiple Csvs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_stats_df.to_csv(path_or_buf=\"graph_stats.csv\",sep=';',float_format='%g',mode='w', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Popular Formulas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (name, df_) in name_to_pop_dfs.items():\n",
    "    df_.to_csv(path_or_buf=name+\".popularity.csv\",sep=';',float_format='%g',mode='w', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution Formulas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (name, df_) in name_to_min_sol_dfs.items():\n",
    "    df_.to_csv(path_or_buf=name+\".solution.csv\",sep=';',float_format='%g',mode='w', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write as Sheets of a Single XLSX File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlsxwriter\n",
    "\n",
    "with pd.ExcelWriter('db_study.xlsx', engine='xlsxwriter') as writer:\n",
    "    workbook = writer.book\n",
    "    for name in sorted(list(name_to_model_ids.keys())):\n",
    "        sheet = workbook.add_worksheet(name=name)\n",
    "\n",
    "        text_wrap = workbook.add_format({'text_wrap': True, 'valign': 'top'})\n",
    "        bold = workbook.add_format({'bold': True})\n",
    "        sheet.set_column(0,0,15)\n",
    "        sheet.set_column(1,1,100,text_wrap)\n",
    "        sheet.set_column(2,2,15)\n",
    "        sheet.set_column(3,3,100,text_wrap)\n",
    "        sheet.set_column(4,4,27)\n",
    "        \n",
    "        row = 0\n",
    "        sheet.merge_range(row,0,row,len(name_to_pop_dfs[name]),\"The 30 most frequent formulas\",bold)\n",
    "        row+=1\n",
    "        name_to_pop_dfs[name].to_excel(excel_writer=writer,sheet_name=name,startrow=row, index=False)\n",
    "        row+= name_to_pop_dfs[name].shape[0] + 2\n",
    "        sheet.merge_range(row,0,row,len(name_to_min_sol_dfs[name]),\"The valid formulas ordered by their frequency\",bold)\n",
    "        row+=1\n",
    "        name_to_min_sol_dfs[name].to_excel(excel_writer=writer,sheet_name=name,startrow=row, index=False)\n",
    "        row+= name_to_min_sol_dfs[name].shape[0] + 2\n",
    "    \n",
    "    graph_stats_df.to_excel(excel_writer=writer,sheet_name=\"General Statistics\", index=False)\n",
    "    workbook.get_worksheet_by_name('General Statistics').set_column(0,0,30)\n",
    "    workbook.get_worksheet_by_name('General Statistics').set_column(1,4,20)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Request hints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import data from file. Checks out the top 10 incorrect most popular submissions  and stores \n",
    "the predicates they belong to. Then, it creates a new dataframe with the top 3 most popular submissions for each one of these predicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# setting the display options\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('max_colwidth', None)\n",
    "\n",
    "# Get the predicates with the most popular incorrect answers \n",
    "challenge = \"dkZH6HJNQNLLDX6Aj\"\n",
    "data = pd.read_csv('popularity.csv', delimiter=';').sort_values(by='Popularity', ascending=False)\n",
    "predicates = data.head(10)[\"Predicate\"].unique().tolist()\n",
    "\n",
    "df = pd.DataFrame()\n",
    "for predicate in predicates:\n",
    "    top = data[data[\"Predicate\"] == predicate].head(3)\n",
    "    df = pd.concat([df, top], ignore_index=True)\n",
    "\n",
    "df.drop(columns=['Popularity'], inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WARNING: Run only after setup is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_body_request(model, challenge, predicate, expression, hintGenType):\n",
    "    obj = {\n",
    "        \"model\": model + \" pred \" + predicate + \" { \" + expression + \" }\",\n",
    "        \"challenge\": challenge,\n",
    "        \"predicate\": predicate,\n",
    "        \"hintGenType\": hintGenType\n",
    "    }\n",
    "\n",
    "    return obj\n",
    "\n",
    "def get_hint(model, url, challenge, predicate, expression, hintGenType = \"TED\"):\n",
    "    body = gen_body_request(model, challenge, predicate, expression, hintGenType)\n",
    "    response = send_http_request(url, body)\n",
    "\n",
    "    if response is not None:\n",
    "        return pd.Series([response[\"hint\"], response[\"nextExpr\"], response[\"targetExpr\"]])\n",
    "    else:\n",
    "        return pd.Series([\"\", \"\", \"\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## HiGenA\n",
    "### TED policy\n",
    "Generates hints using HiGenA with the path with the lowest TED."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"sig User {follows : set User,sees : set Photo,posts : set Photo,suggested : set User} sig Influencer extends User {} sig Photo {date : one Day} sig Ad extends Photo {} sig Day {}\"\n",
    "url = \"http://localhost:8080/hint/higena-hint\"\n",
    "\n",
    "# Hint using Higena with TED\n",
    "higenaTED = df.copy()\n",
    "columns = df.apply(lambda row: get_hint(model, url, row[\"Challenge\"], row[\"Predicate\"], row[\"Expression\"]), axis=1)\n",
    "higenaTED[[\"hint\", \"next\", \"solution\"]] = columns\n",
    "\n",
    "higenaTED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node Popularity policy\n",
    "\n",
    "Generates hints using HiGenA with the path with the most popular submissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint using Higena with Most popular submissions\n",
    "higenaNode = df.copy()\n",
    "columns = df.apply(lambda row: get_hint(model, url, row[\"Challenge\"], row[\"Predicate\"], row[\"Expression\"], hintGenType=\"NODE_POISSON\"), axis=1)\n",
    "higenaNode[[\"hint\", \"next\", \"solution\"]] = columns\n",
    "\n",
    "higenaNode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edge Popularity policy\n",
    "Generates hints using HiGenA with the path with the most popular transitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint using Higena with Most popular submissions\n",
    "higenaEdge = df.copy()\n",
    "columns = df.apply(lambda row: get_hint(model, url, row[\"Challenge\"], row[\"Predicate\"], row[\"Expression\"], hintGenType=\"REL_POISSON\"), axis=1)\n",
    "higenaEdge[[\"hint\", \"next\", \"solution\"]] = columns\n",
    "\n",
    "higenaEdge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Spec Assistant\n",
    "### Default policy\n",
    "Generates hints using SpecAssistant default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute policy\n",
    "url = \"http://localhost:8080/hint/compute-all-policies-for-rule?rule=TEDCOMPXxArrival\"\n",
    "send_http_request(url, [challenge], \"POST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint using Spec Assistant default parameters\n",
    "url = \"http://localhost:8080/hint/spec-hint\"\n",
    "spec = df.copy()\n",
    "columns = df.apply(lambda row: get_hint(model, url, row[\"Challenge\"], row[\"Predicate\"], row[\"Expression\"]), axis=1)\n",
    "spec[[\"hint\", \"next\", \"solution\"]] = columns\n",
    "spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TED policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute policy\n",
    "url = \"http://localhost:8080/hint/compute-all-policies-for-rule?rule=TED\"\n",
    "send_http_request(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate hint\n",
    "url = \"http://localhost:8080/hint/spec-hint\"\n",
    "specTed = df.copy()\n",
    "columns = df.apply(lambda row: get_hint(model, url, row[\"Challenge\"], row[\"Predicate\"], row[\"Expression\"]), axis=1)\n",
    "specTed[[\"hint\", \"next\", \"solution\"]] = columns\n",
    "specTed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node Popularity policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute policy\n",
    "url = \"http://localhost:8080/hint/compute-all-policies-for-rule?rule=MAXFREQ\"\n",
    "send_http_request(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate hint\n",
    "url = \"http://localhost:8080/hint/spec-hint\"\n",
    "specPopularNode = df.copy()\n",
    "columns = df.apply(lambda row: get_hint(model, url, row[\"Challenge\"], row[\"Predicate\"], row[\"Expression\"]), axis=1)\n",
    "specPopularNode[[\"hint\", \"next\", \"solution\"]] = columns\n",
    "specPopularNode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edge Popularity policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute policy\n",
    "url = \"http://localhost:8080/hint/compute-all-policies-for-rule?rule=POPULARITY\"\n",
    "send_http_request(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate hint\n",
    "url = \"http://localhost:8080/hint/spec-hint\"\n",
    "specPopularEdge = df.copy()\n",
    "columns = df.apply(lambda row: get_hint(model, url, row[\"Challenge\"], row[\"Predicate\"], row[\"Expression\"]), axis=1)\n",
    "specPopularEdge[[\"hint\", \"next\", \"solution\"]] = columns\n",
    "specPopularEdge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export hints to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"HiGenATED\"] = higenaTED[\"hint\"]\n",
    "df[\"HiGenAPopularNode\"] = higenaNode[\"hint\"]\n",
    "df[\"HiGenAPopularEdge\"] = higenaEdge[\"hint\"]\n",
    "df[\"specTed\"] = specTed[\"hint\"]\n",
    "df[\"specPopularNode\"] = specPopularNode[\"hint\"]\n",
    "df[\"specPopularEdge\"] = specPopularEdge[\"hint\"]\n",
    "# Export hints csv\n",
    "df.to_csv(\"hints.csv\", index=False, sep=\";\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
